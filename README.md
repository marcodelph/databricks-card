# Pipeline H√≠brido de Detec√ß√£o de Anomalias Financeiras

![Status](https://img.shields.io/badge/status-em%20desenvolvimento-yellow)


## üìñ 1. Resumo

Este projeto consiste na constru√ß√£o de um pipeline de detec√ß√£o de fraude em tempo real, demonstrando uma arquitetura de dados moderna e escal√°vel na nuvem. Utilizando o **Microsoft Azure**, o pipeline ingere um fluxo cont√≠nuo de transa√ß√µes simuladas via **Azure Event Hubs**. O processamento e a transforma√ß√£o dos dados s√£o orquestrados pelo **Azure Databricks**, que emprega **PySpark** e **Spark Structured Streaming** para aplicar a arquitetura Medallion (`raw`, `core`, `analytics`) sobre o **Delta Lake**. A detec√ß√£o de anomalias √© feita com uma abordagem h√≠brida, combinando regras de neg√≥cio em tempo real com a cria√ß√£o de perfis de usu√°rio em batch.

## üéØ 2. O Problema de Neg√≥cio

Sistemas tradicionais de detec√ß√£o de fraude frequentemente se baseiam em regras est√°ticas (ex: "toda transa√ß√£o acima de R$ 5.000 √© suspeita"). Essa abordagem gera muitos falsos positivos e n√£o captura anomalias sutis, pois o que √© "normal" para um usu√°rio pode ser altamente an√¥malo para outro. Este projeto resolve esse problema atrav√©s da detec√ß√£o contextual e individualizada.

## üèóÔ∏è 3. A Solu√ß√£o Arquitetural

A solu√ß√£o foi desenhada sobre dois padr√µes de arquitetura de dados l√≠deres de mercado:

* **Arquitetura H√≠brida (Lambda Simplificada):** Combina um pipeline **batch** para an√°lises profundas e cria√ß√£o de perfis de usu√°rio com um pipeline **streaming** para enriquecimento e alertas em tempo real.
* **Arquitetura Medallion:** Organiza os dados em camadas de qualidade progressiva (`raw`, `core`, `analytics`) dentro do Data Lake, garantindo governan√ßa e rastreabilidade.

## üîÄ 4. Fluxo do Pipeline de Dados

A jornada do dado atrav√©s do pipeline ocorre em quatro etapas principais:

1.  **Ingest√£o (Streaming):** Um script Python (`producer.py`) simula um fluxo cont√≠nuo de transa√ß√µes, com anomalias contextuais injetadas, e as publica no **Azure Event Hubs**.
2.  **Camada Raw:** Um notebook Spark Streaming (`01_streaming_ingestion`) consome os dados do Event Hubs, faz uma limpeza m√≠nima e os persiste em formato Delta na camada `raw` do Data Lake, criando uma fonte da verdade imut√°vel.
3.  **Camada Core:** Um notebook Spark **Batch** (`03_batch_profiling`), orquestrado por um **Databricks Job** di√°rio, l√™ os dados da camada `raw` e cria perfis de comportamento para cada usu√°rio (gasto m√©dio, desvio padr√£o, etc.), salvando o resultado na camada `core`.
4.  **Camada Analytics:** Um segundo notebook Spark Streaming (`02_realtime_alerts`) l√™ as novas transa√ß√µes da camada `raw` e as enriquece em tempo real, atrav√©s de um **stream-table join** com os perfis da camada `core`. Se uma transa√ß√£o desvia significativamente do perfil do usu√°rio, um alerta √© gerado na camada `analytics`.

## üõ†Ô∏è 5. Stack de Tecnologias

| Categoria | Ferramenta/Servi√ßo | Prop√≥sito |
| :--- | :--- | :--- |
| **Plataforma Cloud** | Microsoft Azure | Provedor de todos os servi√ßos de nuvem. |
| **Ingest√£o de Streaming** | Azure Event Hubs | Servi√ßo de mensageria para receber o fluxo de dados. |
| **Armazenamento** | Azure Data Lake Storage Gen2 | Data Lake central para as camadas `raw`, `core`, `analytics`. |
| **Processamento** | Azure Databricks | Plataforma unificada para execu√ß√£o de jobs **PySpark** (incluindo o **Hive Metastore** para gerenciamento de metadados). |
| **Formato dos Dados** | Delta Lake | Formato de tabela que traz transa√ß√µes ACID ao Data Lake. |
| **Seguran√ßa** | Azure Key Vault & Microsoft Entra ID | Gest√£o de segredos e controle de acesso via Service Principal. |
| **Conceitos Fundamentais**| Apache Hadoop | O projeto aplica os conceitos do ecossistema Hadoop em um paradigma moderno: o **ADLS Gen2** como substituto do HDFS para armazenamento distribu√≠do e o **Spark** como motor de processamento. |
| **Linguagens** | Python & SQL | Linguagens usadas para o produtor e para os notebooks Spark. |
| **Dev Tools** | Git & GitHub | Versionamento de c√≥digo. |

## üöÄ 6. Conceitos de DevOps e Boas Pr√°ticas

Este projeto foi constru√≠do com foco em pr√°ticas profissionais de engenharia de software e DevOps:

* **Seguran√ßa e Governan√ßa Robusta (SecOps):** A comunica√ß√£o entre Databricks e Data Lake √© autenticada via **Microsoft Entra ID (Service Principal)**, seguindo o **Princ√≠pio do Menor Privil√©gio** e aplicando uma **governan√ßa de dados robusta** com pap√©is RBAC espec√≠ficos. Segredos s√£o centralizados e gerenciados de forma segura no **Azure Key Vault**.
* **Otimiza√ß√£o de Custos (FinOps):** Todas as decis√µes de infraestrutura foram tomadas com o custo em mente: uso do tier **Standard** do Databricks, cluster **Single Node** que √© desligado automaticamente (`auto-terminate`), e uso do gatilho **`trigger(availableNow=True)`** para economizar recursos de computa√ß√£o.
* **Automa√ß√£o e Orquestra√ß√£o (DevOps/DataOps):** O pipeline foi projetado para ser totalmente automatizado. Os notebooks s√£o orquestrados via **Databricks Jobs**, com o job batch rodando em um cronograma di√°rio, demonstrando pr√°ticas de **DevOps** para o ciclo de vida dos dados.

