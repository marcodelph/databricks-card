# Pipeline de Dados H√≠brido com Azure e Databricks

![Status: Conclu√≠do](https://img.shields.io/badge/status-conclu√≠do-brightgreen)

***

## üéØ Vis√£o Geral do Projeto
Este projeto implementa um pipeline de dados de ponta-a-ponta na nuvem **Azure**, com foco em processamento **h√≠brido (streaming e batch)**. O objetivo foi aplicar as melhores pr√°ticas de engenharia de dados utilizando uma stack moderna, com **Azure Databricks** para processamento com **PySpark**, **Delta Lake** como camada de armazenamento Lakehouse, e **Azure Functions** para ingest√£o serverless.

Este reposit√≥rio serve como um portf√≥lio pr√°tico, demonstrando compet√™ncias em **arquitetura de dados**, processamento distribu√≠do, qualidade de dados e **pr√°ticas de DevOps** para automa√ß√£o e orquestra√ß√£o de pipelines.

***

## üõ†Ô∏è Ferramentas e Tecnologias
| Ferramenta | Prop√≥sito |
| :--- | :--- |
| **Azure Functions** | **Ingest√£o Serverless:** Executa um produtor de eventos agendado para simular um fluxo cont√≠nuo de dados. |
| **Azure Event Hubs** | **Buffer de Streaming:** Servi√ßo de mensageria que recebe o fluxo de dados e o disponibiliza para consumo. |
| **Azure Data Lake Gen2**| **Data Lakehouse:** Camada de armazenamento para as tabelas Delta nas camadas `raw`, `core` e `analytics`. |
| **Azure Databricks** | **Plataforma Unificada:** Ambiente para desenvolvimento, execu√ß√£o e orquestra√ß√£o de todo o pipeline com PySpark. |
| **Delta Lake** | **Camada de Armazenamento:** Formato de tabela que garante transa√ß√µes ACID, confiabilidade e performance ao Data Lake. |
| **PySpark** | **Processamento:** Utilizado para todas as transforma√ß√µes, incluindo **Streaming Estruturado** e l√≥gicas avan√ßadas com **Window Functions**. |
| **PyDeequ** | **Qualidade de Dados:** Framework para definir e executar testes de qualidade de dados de forma declarativa sobre os DataFrames. |
| **Databricks Jobs & Bundles**| **Orquestra√ß√£o & DevOps:** Automa√ß√£o do pipeline (DAG) e defini√ß√£o da infraestrutura do Job como c√≥digo (`databricks.yml`). |
| **Git & GitHub** | **Versionamento:** Sistema para versionamento de todo o c√≥digo do projeto, incluindo notebooks e defini√ß√µes de job. |

***

## üèóÔ∏è Arquitetura da Solu√ß√£o
A solu√ß√£o utiliza uma arquitetura h√≠brida, onde um caminho batch enriquece os dados que s√£o processados em tempo real pelo caminho de streaming. A orquestra√ß√£o √© gerenciada pelo Databricks Jobs, seguindo a arquitetura **Medallion**.

```mermaid
graph TD
    subgraph "Ingest√£o (Azure)"
        A[Azure Function] --> B[Azure Event Hubs];
    end

    subgraph "Processamento (Databricks)"
        P[PySpark Engine]
    end

    subgraph "Armazenamento (ADLS Gen2)"
        E[Raw Layer]
        F[Core Layer]
        H[Analytics Layer]
    end

    B -- "Streaming Ingestion" --> E;
    E -- "Batch Profiling" --> P;
    P -- "MERGE" --> F;
    E -- "Real-time Stream" --> P;
    F -- "Enrichment Join" --> P;
    P -- "Alerts" --> H;
    
    classDef ingest fill:#722bd1,stroke:#333,stroke-width:2px,color:#fff;
    classDef buffer fill:#0078D4,stroke:#333,stroke-width:2px,color:#fff;
    classDef databricks fill:#E25A1C,stroke:#333,stroke-width:2px,color:#fff;
    classDef raw fill:#add8e6,stroke:#333,stroke-width:2px,color:#000;
    classDef core fill:#ff7f50,stroke:#333,stroke-width:2px,color:#000;
    classDef analytics fill:#3cb371,stroke:#333,stroke-width:2px,color:#000;

    class A ingest;
    class B buffer;
    class P databricks;
    class E raw;
    class F core;
    class H analytics;
```
* **Raw (Bronze):** Um pipeline de streaming (`01_ingestion`) consome os dados do Event Hubs e os salva em formato Delta, criando uma c√≥pia fiel da origem.
* **Core (Silver):** Um pipeline batch (`03_profiling`) roda diariamente, lendo os dados da camada `raw` para calcular e atualizar os perfis de comportamento de cada usu√°rio, salvando-os na camada `core`.
* **Analytics (Gold):** Um segundo pipeline de streaming (`02_alerts`) l√™ as novas transa√ß√µes da camada `raw`, as enriquece em tempo real com os perfis da camada `core` (**stream-table join**) e salva os resultados na camada `analytics`.

***

## ‚ú® Destaques de Engenharia com PySpark

* **Engenharia de Features com Window Functions:** Para enriquecer os perfis de usu√°rio, foi utilizada uma **Window Function** (`lag`) no PySpark para calcular o tempo decorrido entre as transa√ß√µes de um mesmo usu√°rio. Essa m√©trica (`avg_time_between_tx_sec`) demonstra a aplica√ß√£o de t√©cnicas avan√ßadas de transforma√ß√£o para criar features anal√≠ticas complexas.
* **Processamento de Streaming com Joins:** A l√≥gica de enriquecimento em tempo real foi implementada atrav√©s de um **join de stream-tabela**, uma t√©cnica do Streaming Estruturado que cruza um fluxo de dados cont√≠nuo com uma tabela de dados est√°tica (os perfis de usu√°rio) para adicionar contexto a cada evento.

***

## ‚úÖ DevOps e Qualidade de Dados

* **Qualidade de Dados com PyDeequ:** A confiabilidade do pipeline foi garantida com a biblioteca **PyDeequ**. Foi implementado um conjunto de testes declarativos que rodam como a etapa final do Job, validando a integridade dos dados na camada `core` (unicidade, valores n√£o-negativos, etc.).
* **Orquestra√ß√£o como C√≥digo (Jobs as Code):** Todo o pipeline, incluindo a sequ√™ncia de tarefas **(DAG)**, a configura√ß√£o dos clusters e as bibliotecas, √© definido como c√≥digo em um arquivo **`databricks.yml`**. Esta abordagem com Databricks Asset Bundles permite que a orquestra√ß√£o seja versionada no Git, garantindo consist√™ncia e reprodutibilidade.
* **CI/CD com GitHub Actions:** O deploy do pipeline √© totalmente automatizado. Um workflow no **GitHub Actions** √© acionado a cada `push` na branch `main`, validando e implantando o `bundle` no Databricks. Isso garante que toda mudan√ßa no c√≥digo passe por um processo de entrega cont√≠nua, eliminando deploys manuais.
* **Seguran√ßa e Governan√ßa:** A comunica√ß√£o entre Databricks e Data Lake √© autenticada via **Microsoft Entra ID (Service Principal)**, seguindo o **Princ√≠pio do Menor Privil√©gio** com pap√©is RBAC espec√≠ficos. Segredos s√£o gerenciados de forma segura nos **GitHub Secrets** e injetados no ambiente de CI/CD.

***

## üöÄ Pr√≥ximos Passos
* **CI/CD com GitHub Actions:** Implementar um workflow no GitHub Actions que, a cada `push` na branch `main`, automaticamente executa o `databricks bundle deploy` para atualizar o Job em produ√ß√£o.
* **Monitoramento:** Configurar alertas no Azure Monitor para notificar sobre falhas no Job ou na Azure Function.
* **Infraestrutura como C√≥digo (IaC):** Utilizar Terraform ou Bicep para provisionar toda a infraestrutura do Azure (Data Lake, Event Hubs, etc.) de forma automatizada.
