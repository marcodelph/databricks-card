{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b8585f-8dc1-43dd-a0f0-a93135d76e9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Importando as bibliotecas necessárias\n",
    "import os\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n",
    "\n",
    "# 1. Ler a connection string \n",
    "connection_string = os.getenv(\"EVENT_HUB_CONNECTION_STRING\") \n",
    "\n",
    "if not connection_string:\n",
    "    raise ValueError(\"EVENT_HUB_CONNECTION_STRING não foi encontrada. Verifique a configuração do cluster.\")\n",
    "\n",
    "# 2. CRIPTOGRAFAR\n",
    "encrypted_connection_string = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n",
    "\n",
    "# 3. Criar a configuração do Event Hub\n",
    "eh_conf = {\n",
    "  'eventhubs.connectionString' : encrypted_connection_string,\n",
    "  'eventhubs.consumerGroup' : '$Default'\n",
    "}\n",
    "\n",
    "# Caminhos para o nosso Data Lake (ADLS Gen2)\n",
    "storage_account_name = \"adlshydra\" \n",
    "raw_path = f\"abfss://raw@{storage_account_name}.dfs.core.windows.net/\"\n",
    "checkpoint_path = f\"abfss://raw@{storage_account_name}.dfs.core.windows.net/checkpoint_ingestion\"\n",
    "\n",
    "# Nome da nossa tabela na camada Raw\n",
    "raw_table_name = \"raw_transactions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35db49ef-504c-44a0-be2c-af822e182cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estrutura dos dados\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"timestamp_utc\", StringType(), False),\n",
    "    StructField(\"user_id\", StringType(), False), # Lendo como String para flexibilidade\n",
    "    StructField(\"card_number\", StringType(), False),\n",
    "    StructField(\"amount_brl\", DoubleType(), False),\n",
    "    StructField(\"merchant_name\", StringType(), False),\n",
    "    StructField(\"merchant_city\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f72b0266-9f3e-4f8b-a846-d61217ffc3b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lendo os dados do Event Hubs como um stream\n",
    "raw_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**eh_conf) \\\n",
    "    .load()\n",
    "\n",
    "# Convertendo o corpo da mensagem (body) para string e aplicando o schema\n",
    "parsed_df = raw_df \\\n",
    "    .select(col(\"body\").cast(\"string\")) \\\n",
    "    .select(from_json(col(\"body\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "parsed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa57a435-2364-4983-9438-955ad0c1fc48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Escrevendo o stream de dados na nossa tabela Delta na camada raw\n",
    "streaming_query = parsed_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(raw_table_name) # vailableNow=True processa todos os dados disponíveis e para"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_streaming_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
